{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5164ce7b",
   "metadata": {},
   "source": [
    "# Notebook Activity: Exploring Automatic Speech Recognition with Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8856f44",
   "metadata": {},
   "source": [
    "Install the required libraries, or use an environment that already includes these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ced2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01976ba7",
   "metadata": {},
   "source": [
    "Select a model (in this case, Whisper-tiny) and try to transcribe a sample audio file. Code provides transcription on CPU, so this may be slow especially for larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-tiny\" # or any other model ID from Hugging Face Hub\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee6d49",
   "metadata": {},
   "source": [
    "Load a sample from the LibriSpeech dataset. \n",
    "\n",
    "You can also use your own audio file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd23716",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[10][\"audio\"]\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6a512",
   "metadata": {},
   "source": [
    "## Activity:\n",
    "Report your results on the following:\n",
    "* Try changing the size of the model. A table of potential model sizes is available on Huggingface [https://huggingface.co/openai/whisper-large-v3]. Note that larger models may require GPU to transcribe in a timely manner.\n",
    "    * How does transcription change with the model size? \n",
    "* Find another audio sample on HuggingFace and test transcription on it, \n",
    "OR record your own audio and pull it into your notebook (note that you should convert this to WAV format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "endo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
